{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d991b63-b578-4cf6-be41-52a1b781d604",
   "metadata": {},
   "source": [
    "# <center>Assignment 4</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838a98-93a7-4d69-b127-d77fa80ff8c8",
   "metadata": {},
   "source": [
    "### 1. Write down the prediction function and cost function and the corresponding python code in the context for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d784230-b405-4661-aec0-fd04ea107348",
   "metadata": {},
   "source": [
    "The <b>sigmoid function</b>, also known as the logistic function, maps any real-valued number to the range (0, 1), which can be used to convert a linear regression output to a probability that the instance belongs to a particular class. Sigmoid function is defined as\n",
    "$$\n",
    "œÉ(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "The <b>prediction function</b> in logistic regression is the sigmoid function of the linear combination of the input features and the model parameters. It can be represented as follows:\n",
    "$$h_Œ∏(x) = \\frac{1}{1 + e^{-(Œ∏^T x)}}$$\n",
    "Where: <br>\n",
    "\n",
    "hŒ∏‚Äã(x) is the predicted output, <br>\n",
    "Œ∏ is the vector of model parameters,<br>\n",
    "x is the vector of input features.<br>\n",
    "The <b>cost function</b> in logistic regression is the log loss function.  It measures the difference between the predicted probabilities and the actual class labels. The goal is to minimize the cost function to find the optimal coefficients for the logistic regression model. It can be represented as follows:\n",
    "$$\n",
    "J(Œ∏) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} log(h_Œ∏(x^{(i)})) + (1 - y^{(i)}) log(1 - h_Œ∏(x^{(i)}))]\n",
    "$$\n",
    "Where:<br>\n",
    "\n",
    "J(Œ∏) is the cost,<br>\n",
    "m is the number of instances in the dataset,<br>\n",
    "y<sup>(i)</sup> is the actual output of the i-th instance,<br>\n",
    "hŒ∏‚Äã(x<sup>(i)</sup>)) is the predicted output of the i-th instance, which is the output of the sigmoid function applied to the linear combination of the input features and the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f96de4-89cc-4beb-b117-46a0de3ef72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def predict(theta, X): #Theta =[w,b]\n",
    "    return sigmoid(np.dot(X, theta)) #or sigmoid(X@w+b)\n",
    "def cost_function(theta, X, y):\n",
    "    m = len(y)\n",
    "    h = predict(theta, X)\n",
    "    cost = -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac1440-bdf9-463d-843f-4fb77a9ce6eb",
   "metadata": {},
   "source": [
    "### 2. Define types of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc109f-3abd-4b22-a5dc-c6bf7c037e2e",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical model used in machine learning for binary classification problems. It can be categorized into three main types: <br> \n",
    "\n",
    "<b>Binary Logistic Regression:</b> This is the most common form of logistic regression, where the response variable (outcome) can only belong to one of two categories. For example, predicting whether an email is spam or not spam.<br><br>\n",
    "\n",
    "<b>Multinomial Logistic Regression:</b> In this type, the response variable can belong to one of three or more categories that do not have a natural ordering. For example, predicting a person‚Äôs preference for a presidential candidate when there are more than two candidates. <br><br>\n",
    "\n",
    "<b>Ordinal Logistic Regression:</b> This type of logistic regression is used when the response variable can belong to one of three or more categories that have a natural ordering. For example, predicting a product‚Äôs rating as low, medium, or high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb48e82-7a56-4c76-a982-f534eb528c53",
   "metadata": {},
   "source": [
    "### 3. List the difference between linear regression and logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dedd2-f26d-4635-ae0e-1bd261c16046",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "1. Linear Regression is a supervised regression model.\n",
    "2. In Linear Regression, we predict the value by an integer number.\n",
    "3. It is based on the least square estimation.\n",
    "4. Here when we plot the training datasets, a straight line can be drawn that touches maximum plots.\n",
    "5. Linear regression is used to estimate the dependent variable in case of a change in independent variables. For example, predict the price of houses.\n",
    "6. Linear regression assumes the normal or gaussian distribution of the dependent variable.\n",
    "\n",
    "#### Logistic Regression\n",
    "1. Logistic Regression is a supervised classification model.\n",
    "2. In Logistic Regression, we predict the value by 1 or 0.\n",
    "3. It is based on maximum likelihood estimation.\n",
    "4. Positive slopes result in an S-shaped curve and negative slopes result in a Z-shaped curve.\n",
    "5. Logistic regression is used to calculate the probability of an event. For example, classify if tissue is benign or malignant.\n",
    "6. Logistic regression assumes the binomial distribution of the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f6e97-b0f4-48ca-ba0c-44fd2eb23c99",
   "metadata": {},
   "source": [
    "### 4. Let you have given the following dataset:\n",
    "x1 x2 y <br>\n",
    "0.5 1 0 <br>\n",
    "1 2 0 <br>\n",
    "1.5 2.5 1 <br>\n",
    "2 3 1 <br>\n",
    "### Where x1, x2 are independent variable and y is dependent variable. In the context of logistic regression, find the optimized parameters after 3rd iteration. Find prediction for [1,1.5] w.r.t. the optimized parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd5582d-2b98-45b5-9ef5-3bbf3e4418ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized parameters:  [-0.00682175  0.06503065  0.07733742]\n",
      "Prediction for [1, 1.5]:  [0.54344393]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" For the following question we will use Gradient descent but manually calculating the \n",
    "parameters after each iteration of Gradient descent can be quite complex and time consuming\n",
    "so we are going to use the code rather than manually solving it\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        predictions = predict(theta, X)\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error)\n",
    "        theta -= alpha * (1/m) * gradient\n",
    "    return theta\n",
    "X = np.array([[0.5, 1], [1, 2], [1.5, 2.5], [2, 3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))  # Add a column of ones for the bias term\n",
    "# Initialize theta\n",
    "theta = np.zeros(X.shape[1])\n",
    "# Set the learning rate and the number of iterations\n",
    "alpha = 0.1\n",
    "iterations = 3\n",
    "theta = gradient_descent(X, y, theta, alpha, iterations)\n",
    "print(\"Optimized parameters: \", theta)\n",
    "# Predict for [1, 1.5]\n",
    "X_new = np.array([1, 1.5]).reshape(1, -1)\n",
    "X_new = np.hstack((np.ones((X_new.shape[0], 1)), X_new))  # Add a column of ones for the bias term\n",
    "prediction = predict(theta, X_new)\n",
    "print(\"Prediction for [1, 1.5]: \", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39be7d9-19b8-445e-8b53-2f568a171bac",
   "metadata": {},
   "source": [
    "### 5. Explain K-Nearest Neighbor (KNN) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8f4cc-2aea-4a73-8d85-756e91842647",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and intuitive classification and regression. It doesn‚Äôt require any mathematical assumptions or discriminative\n",
    "function but memorizes the training dataset instead.\n",
    "\n",
    "\n",
    "1. Let k be the number of neighbours and D be the set of training\n",
    "samples.\n",
    "2. for each test sample t = (x‚Ä≤, y‚Ä≤) do\n",
    "    1. Compute d, the distance between t and training sample, (x, y) ‚àà D.\n",
    "    2. Sort the calculated distances d in ascending order.\n",
    "    3. Get the top k rows from the sorted array.\n",
    "    4. Get the most frequent class corresponding to these rows.\n",
    "    5. Set the class of the test sample to the most frequent class.\n",
    "3. Return the predicted class labels of the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a09d1-40db-42f6-a333-dae60fd7e47a",
   "metadata": {},
   "source": [
    "### 6. How do you choose the optimal k for KNN model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce9d92-ae17-4ef8-8311-e2694317a7f5",
   "metadata": {},
   "source": [
    "1. Cross-Validation like <b> K-fold cross-validation</b> where the data set is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. The average error across all k trials is computed. The optimal choice of k is usually the one that minimizes the test error.<br><br>\n",
    "2. Elbow Method: Run the KNN for a range of k values (say 1 to 20) and plot test error first error will decrease and reach a point (inflection point) and then rise again, option value is that inflection point.<br><br>\n",
    "3. Sometimes, domain knowledge can also help us like in classification of medical condition only closest observations are revelant hence we choose a small k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae39c19-2d1f-4060-94fb-e5ed2def1ced",
   "metadata": {},
   "source": [
    "### 7. Suppose you have given the following dataset:\n",
    "x1 x2 y <br>\n",
    "0.5 0.5 0 <br>\n",
    "0.5 1 0 <br>\n",
    "1 1 0 <br>\n",
    "2 2.5 1 <br>\n",
    "2.5 3 1 <br>\n",
    "3 3 1 <br>\n",
    "### Where x1, x2 are independent variable and y is dependent variable. Predict the class for [1.5,1] for k=2,3 respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0bacb-0ea5-488e-91d2-5d8aebb81e97",
   "metadata": {},
   "source": [
    "Solving KNN manually:\n",
    "\n",
    "1. **Calculate the Euclidean distance** between the point [1.5, 1] and all points in the dataset. The Euclidean distance between two points (x1, y1) and (x2, y2) is given by $\\sqrt{(x1-x2)^2 + (y1-y2)^2}$.\n",
    "\n",
    "2. **Sort the distances** in ascending order and take the first k points.\n",
    "\n",
    "3. **Vote for the class**: For k=2, take the 2 nearest points and use a majority vote to predict the class. For k=3, take the 3 nearest points and do the same.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "- Distances to [1.5, 1]:\n",
    "    - Point [0.5, 0.5]: $\\sqrt{(0.5-1.5)^2 + (0.5-1)^2}$ = 1.118\n",
    "    - Point [0.5, 1]: $\\sqrt{(0.5-1.5)^2 + (1-1)^2}$ = 1.0\n",
    "    - Point [1, 1]: $\\sqrt{(1-1.5)^2 + (1-1)^2}$ = 0.5\n",
    "    - Point [2, 2.5]: $\\sqrt{(2-1.5)^2 + (2.5-1)^2}$ = 1.581\n",
    "    - Point [2.5, 3]: $\\sqrt{(2.5-1.5)^2 + (3-1)^2}$ = 2.236\n",
    "    - Point [3, 3]: $\\sqrt{(3-1.5)^2 + (3-1)^2}$ = 2.5\n",
    "- Sorting wrt Distance:<br>\n",
    " Point &emsp;&emsp; Distance &nbsp;  Class <br>\n",
    "|-----------|---------|------| <br>\n",
    "| [1, 1]&emsp; &ensp; | 0.5 &emsp; &nbsp; | 0 &emsp;| <br>\n",
    "| [0.5, 1] &emsp;| 1.0 &emsp;&ensp;&nbsp;| 0&emsp; |<br>\n",
    "| [0.5, 0.5] &nbsp;| 1.118 &ensp;&nbsp;| 0 &emsp;|<br>\n",
    "| [2, 2.5] &emsp;| 1.581 &ensp;&nbsp;| 1 &emsp;|<br>\n",
    "| [2.5, 3] &emsp;| 2.236 &ensp;&nbsp;| 1 &emsp;|<br>\n",
    "| [3, 3] &emsp;&ensp;&nbsp;| 2.5 &emsp;&ensp;&nbsp;| 1 &emsp;|<br>\n",
    "\n",
    "- For k=2, the two closest points are [1, 1] and [0.5, 1], both of which have class 0. So the prediction for [1.5, 1] is 0.\n",
    "\n",
    "- For k=3, the three closest points are [1, 1], [0.5, 1], and [2, 2.5]. The majority class is 0 (from [1, 1] and [0.5, 1]), so the prediction for [1.5, 1] is also 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa40289-744e-49f3-b08e-a8f67e99e60a",
   "metadata": {},
   "source": [
    "### 8. If the dataset is imbalance, then can the prediction by KNN be bias? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254316e-090b-48f8-a2c7-d5a9a58397f9",
   "metadata": {},
   "source": [
    "<b>Yes</b>, if the dataset is imbalanced, the prediction by K-Nearest Neighbors (KNN) can be biased. Imbalanced datasets occur when one class significantly outnumbers the other class(es). In such cases, the majority class tends to dominate the predictions, leading to biased results. <br><br>\n",
    "Suppose we have a binary classification problem where we want to predict whether a bank transaction is fraudulent (class 1) or not (class 0). However, the dataset is highly imbalanced, with only 1% of transactions being fraudulent (class 1) and 99% being non-fraudulent (class 0). <br><br>\n",
    "\n",
    "<b>Example</b>:Now, let's consider a scenario where we have a new transaction that we want to classify using the KNN algorithm. We choose ùëò = 5 nearest neighbors for simplicity. <br><br>\n",
    "\n",
    "In this imbalanced dataset:<br><br>\n",
    "\n",
    "99% of the neighbors (4 out of 5) may belong to the majority class (non-fraudulent transactions).<br>\n",
    "Only 1% of the neighbors (1 out of 5) may belong to the minority class (fraudulent transactions).<br><br>\n",
    "\n",
    "So even if the new transaction is actually fraudulent it will be classified as non-fraduluent by KNN.<br><br>\n",
    "<center><b> or</b></center> <br>\n",
    "\n",
    "Example: Class A has 900 instances, Class B has 50 with k=3 now even if new instance will be in Class B it can be possible that its three nearest neighbors will all be instances of Class A due to really high majority of Class A,\n",
    "hece putting the new instance in Class A as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cb7bb-16ba-4be1-8ddc-0ff5d1d93b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
